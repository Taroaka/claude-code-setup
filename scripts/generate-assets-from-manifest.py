#!/usr/bin/env python3
"""
Generate assets (image/video/audio) from a `video_manifest.md`.

- Image: Google Gemini Image (Nano Banana Pro = gemini-3-pro-image-preview)
- Video: Google Veo 3.1 (veo-3.1-generate-preview)

Audio (TTS):
- ElevenLabs Text-to-Speech API
"""

from __future__ import annotations

import argparse
import json
import os
import re
import subprocess
import sys
import tempfile
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any

try:
    import yaml  # type: ignore[import-not-found]
except ModuleNotFoundError:  # pragma: no cover
    yaml = None

REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from toc.env import load_env_files
from toc.http import HttpError, request_bytes
from toc.providers.elevenlabs import DEFAULT_ELEVENLABS_VOICE_ID, ElevenLabsClient, ElevenLabsConfig
from toc.providers.gemini import GeminiClient, GeminiConfig
from toc.providers.kling import KlingClient, KlingConfig
from toc.providers.seadream import SeaDreamClient, SeaDreamConfig


ALLOWED_VEO_DURATIONS = (4, 6, 8)


@dataclass
class SceneSpec:
    scene_id: int
    timestamp: str | None
    image_tool: str | None
    image_prompt: str | None
    image_output: str | None
    image_references: list[str]
    image_character_ids: list[str]
    image_character_ids_present: bool
    image_object_ids: list[str]
    image_object_ids_present: bool
    image_aspect_ratio: str | None
    image_size: str | None
    video_tool: str | None
    video_input_image: str | None
    video_first_frame: str | None
    video_last_frame: str | None
    video_motion_prompt: str | None
    video_output: str | None
    narration_tool: str | None
    narration_text: str | None
    narration_output: str | None
    narration_normalize_to_scene_duration: bool


@dataclass(frozen=True)
class CharacterBibleEntry:
    character_id: str | None
    reference_images: list[str]
    fixed_prompts: list[str]
    notes: str | None


@dataclass(frozen=True)
class StyleGuideSpec:
    visual_style: str | None
    forbidden: list[str]
    reference_images: list[str]


@dataclass(frozen=True)
class ObjectBibleEntry:
    object_id: str | None
    kind: str | None  # setpiece|artifact|phenomenon (soft-validated)
    reference_images: list[str]
    fixed_prompts: list[str]
    cinematic_role: str | None
    cinematic_visual_takeaways: list[str]
    cinematic_spectacle_details: list[str]
    notes: str | None


@dataclass(frozen=True)
class AssetGuides:
    character_bible: list[CharacterBibleEntry]
    style_guide: StyleGuideSpec | None
    object_bible: list[ObjectBibleEntry]


def _env(name: str, default: str | None = None) -> str | None:
    v = os.environ.get(name)
    if v is None or v == "":
        return default
    return v


def extract_yaml_block(text: str) -> str:
    m = re.search(r"```yaml\s*\n(.*?)\n```", text, flags=re.DOTALL)
    if not m:
        raise SystemExit("No ```yaml ... ``` block found in manifest markdown.")
    return m.group(1)


def parse_timecode(s: str) -> int:
    s = s.strip()
    parts = s.split(":")
    if len(parts) == 2:
        mm, ss = parts
        return int(mm) * 60 + int(ss)
    if len(parts) == 3:
        hh, mm, ss = parts
        return int(hh) * 3600 + int(mm) * 60 + int(ss)
    raise ValueError(f"Unsupported timecode: {s}")


def duration_from_timestamp_range(ts_range: str | None, default_seconds: int) -> int:
    if not ts_range:
        return default_seconds
    raw = ts_range.strip().strip('"').strip("'")
    if "-" not in raw:
        return default_seconds
    start_s, end_s = raw.split("-", 1)
    try:
        start = parse_timecode(start_s)
        end = parse_timecode(end_s)
    except ValueError:
        return default_seconds
    if end <= start:
        return default_seconds
    return end - start


def _parse_yaml_scalar(value: str) -> str | None:
    v = value.strip()
    if v == "" or v.lower() == "null":
        return None
    if (v.startswith('"') and v.endswith('"')) or (v.startswith("'") and v.endswith("'")):
        return v[1:-1]
    return v


def _ensure_str_list(value: Any) -> list[str]:
    if not value:
        return []
    if isinstance(value, list):
        out: list[str] = []
        for item in value:
            if item is None:
                continue
            s = str(item).strip()
            if not s:
                continue
            out.append(s)
        return out
    s = str(value).strip()
    return [s] if s else []


def _as_opt_str(value: Any) -> str | None:
    if value is None:
        return None
    s = str(value).strip()
    if not s:
        return None
    if s.lower() in {"null", "none"}:
        return None
    return s


def _parse_assets_spec(assets: Any) -> AssetGuides:
    if not isinstance(assets, dict):
        return AssetGuides(character_bible=[], style_guide=None, object_bible=[])

    # character bible
    character_bible: list[CharacterBibleEntry] = []
    raw_cb = assets.get("character_bible")
    if isinstance(raw_cb, list):
        for item in raw_cb:
            if not isinstance(item, dict):
                continue
            character_bible.append(
                CharacterBibleEntry(
                    character_id=_as_opt_str(item.get("character_id")),
                    reference_images=_ensure_str_list(item.get("reference_images")),
                    fixed_prompts=_ensure_str_list(item.get("fixed_prompts")),
                    notes=_as_opt_str(item.get("notes")),
                )
            )

    # style guide
    style_guide = None
    raw_sg = assets.get("style_guide")
    if isinstance(raw_sg, dict):
        style_guide = StyleGuideSpec(
            visual_style=_as_opt_str(raw_sg.get("visual_style")),
            forbidden=_ensure_str_list(raw_sg.get("forbidden")),
            reference_images=_ensure_str_list(raw_sg.get("reference_images")),
        )

    # object / setpiece bible (optional)
    object_bible: list[ObjectBibleEntry] = []
    raw_ob = assets.get("object_bible")
    if isinstance(raw_ob, list):
        for item in raw_ob:
            if not isinstance(item, dict):
                continue
            cinematic = item.get("cinematic") if isinstance(item.get("cinematic"), dict) else {}

            role = (
                _as_opt_str(cinematic.get("role"))
                or _as_opt_str(item.get("cinematic_role"))
                or _as_opt_str(item.get("role_in_film"))
            )
            visual = _ensure_str_list(cinematic.get("visual_takeaways")) or _ensure_str_list(item.get("visual_information"))
            spectacle = _ensure_str_list(cinematic.get("spectacle_details")) or _ensure_str_list(item.get("spectacle_details"))

            object_bible.append(
                ObjectBibleEntry(
                    object_id=_as_opt_str(item.get("object_id")),
                    kind=_as_opt_str(item.get("kind")),
                    reference_images=_ensure_str_list(item.get("reference_images")),
                    fixed_prompts=_ensure_str_list(item.get("fixed_prompts")),
                    cinematic_role=role,
                    cinematic_visual_takeaways=visual,
                    cinematic_spectacle_details=spectacle,
                    notes=_as_opt_str(item.get("notes")),
                )
            )

    return AssetGuides(character_bible=character_bible, style_guide=style_guide, object_bible=object_bible)


def _parse_manifest_yaml_minimal(yaml_text: str) -> tuple[dict, list[SceneSpec]]:
    metadata: dict = {}
    scenes: list[SceneSpec] = []
    current: SceneSpec | None = None

    stack: list[tuple[int, str]] = []

    def push(indent: int, key: str, *, is_list_item: bool) -> None:
        nonlocal stack
        # Support both styles:
        #   scenes:
        #     - scene_id: 1   (indented sequence)
        #   scenes:
        #   - scene_id: 1     (indentless sequence; keep parent key on stack)
        while stack and (indent < stack[-1][0] or (not is_list_item and indent <= stack[-1][0])):
            stack.pop()
        stack.append((indent, key))

    lines = yaml_text.splitlines()
    i = 0
    while i < len(lines):
        raw = lines[i].rstrip("\n")
        if not raw.strip() or raw.lstrip().startswith("#"):
            i += 1
            continue

        indent = len(raw) - len(raw.lstrip(" "))
        stripped = raw.strip()
        is_list_item = stripped.startswith("- ")
        if is_list_item:
            stripped = stripped[2:].strip()

        if ":" not in stripped:
            i += 1
            continue

        key, value = stripped.split(":", 1)
        key = key.strip()
        value = value.strip()

        push(indent, key, is_list_item=is_list_item)
        context_keys = [k for _, k in stack]

        # Block scalar (| / >)
        if value in {"|", "|-", "|+", ">", ">-", ">+"}:
            block_lines: list[str] = []
            j = i + 1
            block_indent: int | None = None
            while j < len(lines):
                nxt = lines[j].rstrip("\n")
                if block_indent is None:
                    if nxt.strip() == "":
                        block_lines.append("")
                        j += 1
                        continue
                    block_indent = len(nxt) - len(nxt.lstrip(" "))

                nxt_indent = len(nxt) - len(nxt.lstrip(" "))
                if nxt.strip() != "" and nxt_indent < block_indent:
                    break
                if nxt.strip() == "" and nxt_indent < block_indent:
                    break

                if nxt.strip() == "":
                    block_lines.append("")
                else:
                    block_lines.append(nxt[block_indent:])
                j += 1

            value = "\n".join(block_lines).rstrip()
            i = j
        else:
            i += 1

        # metadata
        if "video_metadata" in context_keys:
            if key in {"topic", "aspect_ratio", "resolution"}:
                metadata[key] = _parse_yaml_scalar(value)
            continue

        # new scene
        if key == "scene_id" and "scenes" in context_keys:
            if current:
                scenes.append(current)
            try:
                scene_id = int(_parse_yaml_scalar(value) or "0")
            except ValueError:
                scene_id = len(scenes) + 1
            current = SceneSpec(
                scene_id=scene_id,
                timestamp=None,
                image_tool=None,
                image_prompt=None,
                image_output=None,
                image_references=[],
                image_character_ids=[],
                image_character_ids_present=False,
                image_object_ids=[],
                image_object_ids_present=False,
                image_aspect_ratio=None,
                image_size=None,
                video_tool=None,
                video_input_image=None,
                video_first_frame=None,
                video_last_frame=None,
                video_motion_prompt=None,
                video_output=None,
                narration_tool=None,
                narration_text=None,
                narration_output=None,
                narration_normalize_to_scene_duration=True,
            )
            continue

        if not current:
            continue

        # per-scene fields
        if key == "timestamp" and "scenes" in context_keys:
            current.timestamp = _parse_yaml_scalar(value)
            continue

        # image generation
        if "image_generation" in context_keys:
            if key == "tool":
                current.image_tool = _parse_yaml_scalar(value)
            elif key == "prompt":
                current.image_prompt = value if "\n" in value else (_parse_yaml_scalar(value) or value)
            elif key == "output":
                current.image_output = _parse_yaml_scalar(value)
            elif key == "references":
                # Minimal YAML: support inline list syntax like [a, b] only.
                raw = value.strip()
                if raw.startswith("[") and raw.endswith("]"):
                    inner = raw[1:-1].strip()
                    if inner:
                        items = [x.strip().strip('"').strip("'") for x in inner.split(",")]
                        current.image_references = [x for x in items if x]
                else:
                    # Multi-line list parsing is not supported by this minimal parser.
                    current.image_references = []
            elif key == "aspect_ratio":
                current.image_aspect_ratio = _parse_yaml_scalar(value)
            elif key == "image_size":
                current.image_size = _parse_yaml_scalar(value)
            elif key == "character_ids":
                # The minimal parser doesn't support list parsing here; mark presence only.
                current.image_character_ids_present = True
            elif key == "object_ids":
                # The minimal parser doesn't support list parsing here; mark presence only.
                current.image_object_ids_present = True
            continue

        # video generation
        if "video_generation" in context_keys:
            if key == "tool":
                current.video_tool = _parse_yaml_scalar(value)
            elif key == "input_image":
                current.video_input_image = _parse_yaml_scalar(value)
            elif key == "first_frame":
                current.video_first_frame = _parse_yaml_scalar(value)
            elif key == "last_frame":
                current.video_last_frame = _parse_yaml_scalar(value)
            elif key == "motion_prompt":
                current.video_motion_prompt = value if "\n" in value else (_parse_yaml_scalar(value) or value)
            elif key == "output":
                current.video_output = _parse_yaml_scalar(value)
            continue

        # narration
        if "narration" in context_keys:
            if key == "tool":
                current.narration_tool = _parse_yaml_scalar(value)
            elif key == "text":
                current.narration_text = value if "\n" in value else (_parse_yaml_scalar(value) or value)
            elif key == "output":
                current.narration_output = _parse_yaml_scalar(value)
            elif key == "normalize_to_scene_duration":
                raw = (_parse_yaml_scalar(value) or "").strip().lower()
                if raw in {"false", "no", "0"}:
                    current.narration_normalize_to_scene_duration = False
            continue

    if current:
        scenes.append(current)

    return metadata, scenes


def _parse_manifest_yaml_pyyaml(yaml_text: str) -> tuple[dict, AssetGuides, list[SceneSpec]]:
    if yaml is None:  # pragma: no cover
        raise RuntimeError("PyYAML is not installed.")

    data = yaml.safe_load(yaml_text)
    if not isinstance(data, dict):
        raise ValueError("Manifest YAML must be a mapping at the root.")

    vm = data.get("video_metadata")
    metadata = {}
    if isinstance(vm, dict):
        for key in ("topic", "aspect_ratio", "resolution"):
            if key in vm:
                metadata[key] = _as_opt_str(vm.get(key))

    assets = _parse_assets_spec(data.get("assets"))

    scenes: list[SceneSpec] = []
    raw_scenes = data.get("scenes") or []
    if not isinstance(raw_scenes, list):
        raise ValueError("Manifest YAML scenes must be a list.")
    for raw_scene in raw_scenes:
        if not isinstance(raw_scene, dict):
            continue

        try:
            scene_id = int(raw_scene.get("scene_id"))
        except Exception:
            continue

        timestamp = _as_opt_str(raw_scene.get("timestamp"))

        raw_cuts = raw_scene.get("cuts")
        if isinstance(raw_cuts, list) and raw_cuts:
            for idx, raw_cut in enumerate(raw_cuts, start=1):
                if not isinstance(raw_cut, dict):
                    continue

                cut_id_raw = raw_cut.get("cut_id")
                try:
                    cut_id = int(cut_id_raw) if cut_id_raw is not None else int(idx)
                except Exception:
                    cut_id = int(idx)

                cut_scene_id = int(scene_id) * 100 + int(cut_id)

                ig = raw_cut.get("image_generation") if isinstance(raw_cut.get("image_generation"), dict) else {}
                vg = raw_cut.get("video_generation") if isinstance(raw_cut.get("video_generation"), dict) else {}

                image_tool = _as_opt_str(ig.get("tool")) if isinstance(ig, dict) else None
                image_prompt = _as_opt_str(ig.get("prompt")) if isinstance(ig, dict) else None
                image_output = _as_opt_str(ig.get("output")) if isinstance(ig, dict) else None
                image_references = _ensure_str_list(ig.get("references")) if isinstance(ig, dict) else []
                image_character_ids_present = isinstance(ig, dict) and ("character_ids" in ig)
                image_character_ids = _ensure_str_list(ig.get("character_ids")) if isinstance(ig, dict) else []
                image_object_ids_present = isinstance(ig, dict) and ("object_ids" in ig)
                image_object_ids = _ensure_str_list(ig.get("object_ids")) if isinstance(ig, dict) else []
                image_aspect_ratio = _as_opt_str(ig.get("aspect_ratio")) if isinstance(ig, dict) else None
                image_size = _as_opt_str(ig.get("image_size")) if isinstance(ig, dict) else None

                video_tool = _as_opt_str(vg.get("tool")) if isinstance(vg, dict) else None
                video_input_image = _as_opt_str(vg.get("input_image")) if isinstance(vg, dict) else None
                video_first_frame = _as_opt_str(vg.get("first_frame")) if isinstance(vg, dict) else None
                video_last_frame = _as_opt_str(vg.get("last_frame")) if isinstance(vg, dict) else None
                video_motion_prompt = _as_opt_str(vg.get("motion_prompt")) if isinstance(vg, dict) else None
                video_output = _as_opt_str(vg.get("output")) if isinstance(vg, dict) else None

                narration_tool = None
                narration_text = None
                narration_output = None
                narration_normalize = True

                audio = raw_cut.get("audio")
                narration = None
                if isinstance(audio, dict):
                    narration = audio.get("narration")
                if narration is None:
                    narration = raw_cut.get("narration")
                if isinstance(narration, dict):
                    narration_tool = _as_opt_str(narration.get("tool"))
                    narration_text = _as_opt_str(narration.get("text"))
                    narration_output = _as_opt_str(narration.get("output"))
                    normalize_raw = narration.get("normalize_to_scene_duration")
                    if isinstance(normalize_raw, bool):
                        narration_normalize = bool(normalize_raw)
                    else:
                        normalize_s = _as_opt_str(normalize_raw)
                        if normalize_s and normalize_s.strip().lower() in {"false", "no", "0"}:
                            narration_normalize = False

                scenes.append(
                    SceneSpec(
                        scene_id=cut_scene_id,
                        timestamp=timestamp,
                        image_tool=image_tool,
                        image_prompt=image_prompt,
                        image_output=image_output,
                        image_references=image_references,
                        image_character_ids=image_character_ids,
                        image_character_ids_present=image_character_ids_present,
                        image_object_ids=image_object_ids,
                        image_object_ids_present=image_object_ids_present,
                        image_aspect_ratio=image_aspect_ratio,
                        image_size=image_size,
                        video_tool=video_tool,
                        video_input_image=video_input_image,
                        video_first_frame=video_first_frame,
                        video_last_frame=video_last_frame,
                        video_motion_prompt=video_motion_prompt,
                        video_output=video_output,
                        narration_tool=narration_tool,
                        narration_text=narration_text,
                        narration_output=narration_output,
                        narration_normalize_to_scene_duration=narration_normalize,
                    )
                )
            continue

        ig = raw_scene.get("image_generation") if isinstance(raw_scene.get("image_generation"), dict) else {}
        vg = raw_scene.get("video_generation") if isinstance(raw_scene.get("video_generation"), dict) else {}

        image_tool = _as_opt_str(ig.get("tool")) if isinstance(ig, dict) else None
        image_prompt = _as_opt_str(ig.get("prompt")) if isinstance(ig, dict) else None
        image_output = _as_opt_str(ig.get("output")) if isinstance(ig, dict) else None
        image_references = _ensure_str_list(ig.get("references")) if isinstance(ig, dict) else []
        image_character_ids_present = isinstance(ig, dict) and ("character_ids" in ig)
        image_character_ids = _ensure_str_list(ig.get("character_ids")) if isinstance(ig, dict) else []
        image_object_ids_present = isinstance(ig, dict) and ("object_ids" in ig)
        image_object_ids = _ensure_str_list(ig.get("object_ids")) if isinstance(ig, dict) else []
        image_aspect_ratio = _as_opt_str(ig.get("aspect_ratio")) if isinstance(ig, dict) else None
        image_size = _as_opt_str(ig.get("image_size")) if isinstance(ig, dict) else None

        video_tool = _as_opt_str(vg.get("tool")) if isinstance(vg, dict) else None
        video_input_image = _as_opt_str(vg.get("input_image")) if isinstance(vg, dict) else None
        video_first_frame = _as_opt_str(vg.get("first_frame")) if isinstance(vg, dict) else None
        video_last_frame = _as_opt_str(vg.get("last_frame")) if isinstance(vg, dict) else None
        video_motion_prompt = _as_opt_str(vg.get("motion_prompt")) if isinstance(vg, dict) else None
        video_output = _as_opt_str(vg.get("output")) if isinstance(vg, dict) else None

        # narration can be nested under audio.narration or directly under narration (legacy)
        narration_tool = None
        narration_text = None
        narration_output = None
        narration_normalize = True

        audio = raw_scene.get("audio")
        narration = None
        if isinstance(audio, dict):
            narration = audio.get("narration")
        if narration is None:
            narration = raw_scene.get("narration")
        if isinstance(narration, dict):
            narration_tool = _as_opt_str(narration.get("tool"))
            narration_text = _as_opt_str(narration.get("text"))
            narration_output = _as_opt_str(narration.get("output"))
            normalize_raw = narration.get("normalize_to_scene_duration")
            if isinstance(normalize_raw, bool):
                narration_normalize = bool(normalize_raw)
            else:
                normalize_s = _as_opt_str(normalize_raw)
                if normalize_s and normalize_s.strip().lower() in {"false", "no", "0"}:
                    narration_normalize = False

        scenes.append(
            SceneSpec(
                scene_id=scene_id,
                timestamp=timestamp,
                image_tool=image_tool,
                image_prompt=image_prompt,
                image_output=image_output,
                image_references=image_references,
                image_character_ids=image_character_ids,
                image_character_ids_present=image_character_ids_present,
                image_object_ids=image_object_ids,
                image_object_ids_present=image_object_ids_present,
                image_aspect_ratio=image_aspect_ratio,
                image_size=image_size,
                video_tool=video_tool,
                video_input_image=video_input_image,
                video_first_frame=video_first_frame,
                video_last_frame=video_last_frame,
                video_motion_prompt=video_motion_prompt,
                video_output=video_output,
                narration_tool=narration_tool,
                narration_text=narration_text,
                narration_output=narration_output,
                narration_normalize_to_scene_duration=narration_normalize,
            )
        )

    return metadata, assets, scenes


def parse_manifest_yaml_full(yaml_text: str) -> tuple[dict, AssetGuides, list[SceneSpec]]:
    try:
        return _parse_manifest_yaml_pyyaml(yaml_text)
    except Exception:
        metadata, scenes = _parse_manifest_yaml_minimal(yaml_text)
        return metadata, AssetGuides(character_bible=[], style_guide=None, object_bible=[]), scenes


def parse_manifest_yaml(yaml_text: str) -> tuple[dict, list[SceneSpec]]:
    metadata, _, scenes = parse_manifest_yaml_full(yaml_text)
    return metadata, scenes


def _dedupe_keep_order(items: list[str]) -> list[str]:
    seen: set[str] = set()
    out: list[str] = []
    for item in items:
        s = str(item).strip()
        if not s:
            continue
        if s in seen:
            continue
        seen.add(s)
        out.append(s)
    return out


def _merge_refs(existing: list[str], extra: list[str], *, exclude: str | None = None) -> list[str]:
    seen: set[str] = set()
    out: list[str] = []

    def add_one(v: str) -> None:
        s = str(v).strip()
        if not s:
            return
        if exclude and s == exclude:
            return
        if s in seen:
            return
        seen.add(s)
        out.append(s)

    for v in existing or []:
        add_one(v)
    for v in extra or []:
        add_one(v)
    return out


_HEADING_ALIASES: dict[str, list[str]] = {
    # Keep canonical English keys for code, but accept Japanese headings in prompts/templates.
    "GLOBAL / INVARIANTS": ["GLOBAL / INVARIANTS", "全体 / 不変条件", "全体/不変条件", "グローバル / 不変条件"],
    "CHARACTERS": ["CHARACTERS", "登場人物", "キャラクター"],
    "PROPS / SETPIECES": ["PROPS / SETPIECES", "小道具 / 舞台装置", "小道具/舞台装置", "プロップ / 舞台装置"],
    "SCENE": ["SCENE", "シーン", "場面"],
    "CONTINUITY": ["CONTINUITY", "連続性", "つながり"],
    "AVOID": ["AVOID", "禁止", "避けること", "NG"],
}

_HEADING_JA_LABEL: dict[str, str] = {
    "GLOBAL / INVARIANTS": "全体 / 不変条件",
    "CHARACTERS": "登場人物",
    "PROPS / SETPIECES": "小道具 / 舞台装置",
    "SCENE": "シーン",
    "CONTINUITY": "連続性",
    "AVOID": "禁止",
}


def _find_heading_line_index(lines: list[str], heading: str) -> int | None:
    candidates = _HEADING_ALIASES.get(heading, [heading])
    targets = {f"[{h}]" for h in candidates}
    for i, line in enumerate(lines):
        if line.strip() in targets:
            return i
    return None


def _inject_lines_under_heading(prompt: str, heading: str, lines_to_add: list[str]) -> str:
    if not prompt:
        prompt = ""
    lines = prompt.splitlines()
    existing = {ln.strip() for ln in lines}
    to_add = [ln.strip() for ln in lines_to_add if str(ln).strip() and str(ln).strip() not in existing]
    if not to_add:
        return prompt

    idx = _find_heading_line_index(lines, heading)
    if idx is None:
        # No structured heading: append a new section at the end.
        label = _HEADING_JA_LABEL.get(heading, heading)
        suffix = "\n".join([f"[{label}]", *to_add])
        if prompt.strip() == "":
            return suffix
        return (prompt.rstrip() + "\n\n" + suffix).rstrip()

    insert_at = idx + 1
    lines[insert_at:insert_at] = to_add
    return "\n".join(lines).rstrip()


def _asset_guides_character_refs_to_add(guides: AssetGuides, mode: str) -> list[str]:
    mode_norm = (mode or "").strip().lower()
    if mode_norm == "none":
        return []
    if mode_norm == "scene":
        return []
    if mode_norm == "all":
        refs: list[str] = []
        for entry in guides.character_bible:
            refs.extend(entry.reference_images or [])
        return _dedupe_keep_order(refs)

    # auto: only apply when there's exactly one character entry (avoids accidentally mixing multiple identities)
    if len(guides.character_bible) == 1:
        return _dedupe_keep_order(list(guides.character_bible[0].reference_images or []))
    return []


def apply_asset_guides_to_scene(*, scene: SceneSpec, guides: AssetGuides, character_refs_mode: str) -> None:
    """
    Mutates scene in-place:
    - merges assets.* reference images into scene.image_references
    - injects assets.* prompt lines into scene.image_prompt (best-effort; uses headings when present)

    This is an opt-in helper intended to reduce per-scene copy/paste while keeping prompts structured.
    """

    # references
    style_refs = []
    if guides.style_guide:
        style_refs = guides.style_guide.reference_images or []

    mode_norm = (character_refs_mode or "").strip().lower()
    if mode_norm == "scene":
        if scene.image_character_ids:
            chosen = set(scene.image_character_ids)
            char_refs = _dedupe_keep_order(
                [ref for entry in guides.character_bible for ref in (entry.reference_images or []) if entry.character_id in chosen]
            )
        else:
            char_refs = []
    else:
        char_refs = _asset_guides_character_refs_to_add(guides, character_refs_mode)

    merged_refs = _merge_refs(scene.image_references or [], style_refs, exclude=scene.image_output)
    merged_refs = _merge_refs(merged_refs, char_refs, exclude=scene.image_output)

    # Add object/setpiece reference images based on explicit per-scene object_ids.
    obj_refs: list[str] = []
    chosen_object_ids = set(scene.image_object_ids or [])
    if chosen_object_ids:
        obj_refs = _dedupe_keep_order(
            [
                ref
                for entry in (guides.object_bible or [])
                for ref in (entry.reference_images or [])
                if entry.object_id in chosen_object_ids
            ]
        )
    merged_refs = _merge_refs(merged_refs, obj_refs, exclude=scene.image_output)
    scene.image_references = merged_refs

    # prompt injection
    if not scene.image_prompt:
        return

    prompt = scene.image_prompt

    global_lines: list[str] = []
    if guides.style_guide and guides.style_guide.visual_style:
        global_lines.append(guides.style_guide.visual_style)

    avoid_lines: list[str] = []
    if guides.style_guide and guides.style_guide.forbidden:
        avoid_lines.extend(guides.style_guide.forbidden)

    # Inject character fixed prompts only when that character is "active" for the scene:
    # - either its reference images are used, or this scene is generating that reference image.
    char_lines: list[str] = []
    ref_set = set(merged_refs)
    chosen_ids = set(scene.image_character_ids or [])
    for entry in guides.character_bible or []:
        if mode_norm == "scene" and chosen_ids:
            is_active = entry.character_id in chosen_ids
        else:
            is_active = any(ref in ref_set for ref in (entry.reference_images or []))
        if not is_active and scene.image_output and scene.image_output in (entry.reference_images or []):
            is_active = True
        if is_active and entry.fixed_prompts:
            char_lines.extend(entry.fixed_prompts)

    # Inject object/setpiece prompts only when that object is active for the scene.
    prop_lines: list[str] = []
    for entry in guides.object_bible or []:
        if not entry.object_id:
            continue

        is_active = entry.object_id in chosen_object_ids
        if not is_active:
            is_active = any(ref in ref_set for ref in (entry.reference_images or []))
        if not is_active and scene.image_output and scene.image_output in (entry.reference_images or []):
            is_active = True
        if not is_active:
            continue

        if entry.fixed_prompts:
            prop_lines.extend(entry.fixed_prompts)
        if entry.cinematic_role:
            prop_lines.append(f"映画での役割: {entry.cinematic_role}")
        for v in entry.cinematic_visual_takeaways or []:
            prop_lines.append(f"映像から伝える情報: {v}")
        for s in entry.cinematic_spectacle_details or []:
            prop_lines.append(f"見せ場ディテール: {s}")

    if global_lines:
        prompt = _inject_lines_under_heading(prompt, "GLOBAL / INVARIANTS", global_lines)
    if char_lines:
        prompt = _inject_lines_under_heading(prompt, "CHARACTERS", char_lines)
    if prop_lines:
        prompt = _inject_lines_under_heading(prompt, "PROPS / SETPIECES", prop_lines)
    if avoid_lines:
        prompt = _inject_lines_under_heading(prompt, "AVOID", avoid_lines)

    scene.image_prompt = prompt


def validate_scene_character_ids(
    *, scenes: list[SceneSpec], require: bool, mode: str, scene_filter: set[int] | None
) -> None:
    if not require:
        return
    if (mode or "").strip().lower() != "scene":
        return
    for scene in scenes:
        if scene_filter is not None and int(scene.scene_id) not in scene_filter:
            continue
        if not scene.image_output or not scene.image_prompt:
            continue
        if not scene.image_character_ids_present:
            raise SystemExit(
                f"scene{scene.scene_id}: missing image_generation.character_ids. "
                "For B-roll scenes, set an explicit empty list: character_ids: []."
            )


def validate_scene_object_ids(
    *, scenes: list[SceneSpec], guides: AssetGuides, require: bool, scene_filter: set[int] | None
) -> None:
    if not require:
        return
    if not guides.object_bible:
        return
    for scene in scenes:
        if scene_filter is not None and int(scene.scene_id) not in scene_filter:
            continue
        if not scene.image_output or not scene.image_prompt:
            continue
        if not scene.image_object_ids_present:
            raise SystemExit(
                f"scene{scene.scene_id}: missing image_generation.object_ids. "
                "For scenes with no props/setpieces, set an explicit empty list: object_ids: []."
            )


def validate_object_reference_scenes(*, scenes: list[SceneSpec], guides: AssetGuides, require: bool) -> None:
    if not require:
        return
    if not guides.object_bible:
        return

    outputs = {str(s.image_output) for s in scenes if s.image_output}

    missing_required: list[str] = []
    missing_outputs: list[str] = []
    for entry in guides.object_bible or []:
        if not entry.object_id:
            missing_required.append("object_id is required (found null/empty).")
            continue
        if not entry.reference_images:
            missing_required.append(f"{entry.object_id}: reference_images is required and must be non-empty.")
        if not entry.fixed_prompts:
            missing_required.append(f"{entry.object_id}: fixed_prompts is required and must be non-empty.")

        for ref in entry.reference_images or []:
            if ref not in outputs:
                missing_outputs.append(f"{entry.object_id}:{ref}")

    if missing_required:
        raise SystemExit("assets.object_bible invalid:\n- " + "\n- ".join(missing_required))
    if missing_outputs:
        raise SystemExit(
            "Missing object reference scenes: each assets.object_bible[].reference_images path must be generated "
            "by some scenes[].image_generation.output.\n- " + "\n- ".join(missing_outputs)
        )


def _guess_image_suffix(mime_type: str | None) -> str:
    if not mime_type:
        return ".bin"
    mt = mime_type.lower()
    if mt == "image/png":
        return ".png"
    if mt == "image/jpeg":
        return ".jpg"
    if mt == "image/webp":
        return ".webp"
    return ".bin"


def _run(cmd: list[str]) -> None:
    subprocess.run(cmd, check=True)


def _parse_csv_set(value: str | None) -> set[str]:
    if not value:
        return set()
    items = []
    for part in str(value).split(","):
        s = part.strip().lower()
        if not s:
            continue
        items.append(s)
    return set(items)


def _is_character_ref_path(path: Path) -> bool:
    try:
        return path.parent.name == "characters" and path.parent.parent.name == "assets"
    except Exception:
        return False


def _is_object_ref_path(path: Path) -> bool:
    try:
        return path.parent.name == "objects" and path.parent.parent.name == "assets"
    except Exception:
        return False


def _derive_character_view_path(front_path: Path, view: str) -> Path:
    """
    Derive a sibling filename for a character reference view.

    Supports both:
    - protagonist.png -> protagonist_side.png / protagonist_back.png
    - protagonist_front.png -> protagonist_side.png / protagonist_back.png
    """
    view = (view or "").strip().lower()
    if view == "front":
        return front_path

    suffix = front_path.suffix or ".png"
    stem = front_path.stem
    if stem.endswith("_front"):
        root = stem[: -len("_front")]
        return front_path.with_name(f"{root}_{view}{suffix}")
    if stem.endswith(f"_{view}"):
        return front_path
    return front_path.with_name(f"{stem}_{view}{suffix}")


def _derive_character_refstrip_path(front_path: Path, strip_suffix: str) -> Path:
    suffix = front_path.suffix or ".png"
    stem = front_path.stem
    root = stem
    for v in ("_front", "_side", "_back"):
        if root.endswith(v):
            root = root[: -len(v)]
            break
    return front_path.with_name(f"{root}{strip_suffix}{suffix}")


def _is_character_refstrip_path(path: Path, strip_suffix: str) -> bool:
    if not _is_character_ref_path(path):
        return False
    suff = (strip_suffix or "").strip()
    if not suff:
        return False
    return path.stem.endswith(suff)


def _ffmpeg_hstack_images(inputs: list[Path], out_path: Path, *, force: bool) -> None:
    if out_path.exists() and not force:
        return
    if len(inputs) < 2:
        raise ValueError("hstack requires at least 2 inputs")
    out_path.parent.mkdir(parents=True, exist_ok=True)
    cmd = ["ffmpeg", "-hide_banner", "-y"]
    for p in inputs:
        cmd += ["-i", str(p)]
    cmd += [
        "-filter_complex",
        f"hstack=inputs={len(inputs)}",
        "-frames:v",
        "1",
        "-update",
        "1",
        str(out_path),
    ]
    _run(cmd)


def _character_view_prompt(base_prompt: str, view: str) -> str:
    view_norm = (view or "").strip().lower()
    if view_norm not in {"front", "side", "back"}:
        return base_prompt

    if view_norm == "front":
        view_lines = [
            "キャラクター参照画像: 正面（FRONT）ビュー。",
            "全身（頭からつま先まで）を入れる。足先が切れない（クロップしない）。",
            "ニュートラルな姿勢。腕は自然に下ろす。中央構図。背景はクリーンで無地。",
        ]
    elif view_norm == "side":
        view_lines = [
            "キャラクター参照画像: 左側面（LEFT SIDE）ビュー。",
            "全身（頭からつま先まで）を入れる。足先が切れない（クロップしない）。",
            "ニュートラルな姿勢。中央構図。背景はクリーンで無地。",
        ]
    else:  # back
        view_lines = [
            "キャラクター参照画像: 背面（BACK）ビュー。",
            "全身（頭からつま先まで）を入れる。足先が切れない（クロップしない）。",
            "ニュートラルな姿勢。中央構図。背景はクリーンで無地。",
        ]

    # Prefer structured injection under [SCENE]; fall back to appending.
    return _inject_lines_under_heading(base_prompt, "SCENE", view_lines)


def _ffmpeg_write_silence_mp3(out_path: Path, duration_seconds: int, force: bool) -> None:
    if out_path.exists() and not force:
        return
    out_path.parent.mkdir(parents=True, exist_ok=True)
    _run(
        [
            "ffmpeg",
            "-hide_banner",
            "-y",
            "-f",
            "lavfi",
            "-i",
            "anullsrc=r=44100:cl=mono",
            "-t",
            str(duration_seconds),
            "-q:a",
            "9",
            "-acodec",
            "libmp3lame",
            str(out_path),
        ]
    )


def _ffmpeg_normalize_mp3(src_path: Path, out_path: Path, duration_seconds: int | None, force: bool) -> None:
    if out_path.exists() and not force:
        return
    out_path.parent.mkdir(parents=True, exist_ok=True)
    cmd = [
        "ffmpeg",
        "-hide_banner",
        "-y",
        "-i",
        str(src_path),
        "-ar",
        "44100",
        "-ac",
        "1",
        "-b:a",
        "128k",
        "-codec:a",
        "libmp3lame",
    ]
    if duration_seconds is not None:
        cmd += ["-af", "apad", "-t", str(duration_seconds)]
    cmd.append(str(out_path))
    _run(cmd)


def generate_elevenlabs_tts(
    *,
    client: ElevenLabsClient | None,
    voice_id: str,
    model_id: str,
    output_format: str,
    text: str,
    out_path: Path,
    duration_seconds: int | None,
    force: bool,
    request_log_path: Path | None,
    dry_run: bool,
) -> None:
    if out_path.exists() and not force:
        return

    payload: dict = {
        "text": text,
        "model_id": model_id,
        "voice_settings": {
            "stability": 0.35,
            "similarity_boost": 0.75,
            "style": 0.0,
            "use_speaker_boost": True,
        },
    }

    if request_log_path:
        request_log_path.parent.mkdir(parents=True, exist_ok=True)
        request_log_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")

    if dry_run:
        print(f"[dry-run] AUDIO {out_path} <- elevenlabs voice={voice_id} model={model_id} fmt={output_format}")
        return

    if client is None:
        raise SystemExit("ElevenLabs client not configured (missing ELEVENLABS_API_KEY).")

    try:
        audio = client.tts(
            text=text,
            voice_id=voice_id,
            model_id=model_id,
            output_format=output_format,
            voice_settings=payload["voice_settings"],
        )
    except HttpError as e:
        raise SystemExit(str(e)) from e

    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp:
        tmp_path = Path(tmp.name)
        tmp.write(audio)

    try:
        try:
            _ffmpeg_normalize_mp3(tmp_path, out_path, duration_seconds, force=True)
        except FileNotFoundError:
            out_path.parent.mkdir(parents=True, exist_ok=True)
            out_path.write_bytes(audio)
    finally:
        try:
            tmp_path.unlink(missing_ok=True)
        except Exception:
            pass


def _plan_veo_segments(desired_seconds: int) -> tuple[list[int], int | None]:
    """
    Return (segments, trim_to_seconds).

    Veo only supports a small discrete set of durations per request; if the desired duration
    isn't directly supported, we generate multiple segments and trim the concatenation.
    """
    if desired_seconds <= 0:
        return [6], 6

    if desired_seconds in ALLOWED_VEO_DURATIONS:
        return [desired_seconds], None

    limit = desired_seconds + max(ALLOWED_VEO_DURATIONS)
    best: dict[int, list[int]] = {0: []}
    for total in range(limit + 1):
        if total not in best:
            continue
        for d in ALLOWED_VEO_DURATIONS:
            nxt = total + d
            if nxt > limit:
                continue
            cand = best[total] + [d]
            if nxt not in best or len(cand) < len(best[nxt]):
                best[nxt] = cand

    best_total = None
    best_segments: list[int] | None = None
    for total in range(desired_seconds, limit + 1):
        segs = best.get(total)
        if not segs:
            continue
        if best_total is None:
            best_total = total
            best_segments = segs
            continue
        overshoot = total - desired_seconds
        best_overshoot = best_total - desired_seconds
        if overshoot < best_overshoot:
            best_total = total
            best_segments = segs
        elif overshoot == best_overshoot and best_segments is not None and len(segs) < len(best_segments):
            best_total = total
            best_segments = segs

    if not best_segments:
        return [6], desired_seconds

    if best_total == desired_seconds:
        return best_segments, None
    return best_segments, desired_seconds


def _ffmpeg_concat_videos(inputs: list[Path], out_path: Path) -> None:
    out_path.parent.mkdir(parents=True, exist_ok=True)
    with tempfile.TemporaryDirectory() as tmpdir:
        list_path = Path(tmpdir) / "concat.txt"
        lines = [f"file '{p.as_posix()}'" for p in inputs]
        list_path.write_text("\n".join(lines) + "\n", encoding="utf-8")
        _run(
            [
                "ffmpeg",
                "-hide_banner",
                "-y",
                "-f",
                "concat",
                "-safe",
                "0",
                "-i",
                str(list_path),
                "-c",
                "copy",
                str(out_path),
            ]
        )


def _ffmpeg_trim_video(src: Path, out_path: Path, duration_seconds: int) -> None:
    out_path.parent.mkdir(parents=True, exist_ok=True)
    _run(
        [
            "ffmpeg",
            "-hide_banner",
            "-y",
            "-i",
            str(src),
            "-t",
            str(duration_seconds),
            "-c:v",
            "libx264",
            "-preset",
            "medium",
            "-crf",
            "18",
            "-pix_fmt",
            "yuv420p",
            "-c:a",
            "aac",
            "-b:a",
            "192k",
            str(out_path),
        ]
    )


def _ffmpeg_extract_frame_from_end(src: Path, out_path: Path, *, seconds_from_end: float, force: bool) -> None:
    if out_path.exists() and not force:
        return
    # ffmpeg can fail to output any frame if we seek *too* close to EOF.
    # For 24fps content, 1 frame ~= 0.0417s; treat that as "last frame" in practice.
    min_seek = 1.0 / 24.0
    if seconds_from_end <= 0:
        seconds_from_end = min_seek
    if seconds_from_end < min_seek:
        seconds_from_end = min_seek
    out_path.parent.mkdir(parents=True, exist_ok=True)
    _run(
        [
            "ffmpeg",
            "-hide_banner",
            "-y",
            "-sseof",
            f"-{seconds_from_end}",
            "-i",
            str(src),
            "-frames:v",
            "1",
            "-q:v",
            "2",
            str(out_path),
        ]
    )


def _ffmpeg_extract_frame_from_end_best_effort(
    src: Path, out_path: Path, *, seconds_from_end: float, force: bool
) -> Path:
    """
    Extract a "near end" frame reliably.

    ffmpeg can exit 0 but still write an empty file if the seek is too close to EOF.
    We retry with progressively larger offsets.
    """
    if out_path.exists() and not force and out_path.stat().st_size > 0:
        return out_path

    min_seek = 1.0 / 24.0
    candidates: list[float] = [max(float(seconds_from_end), min_seek)]
    candidates += [min_seek, 0.05, 0.1, 0.25, 0.5, 1.0]

    last_err: Exception | None = None
    for sec in candidates:
        try:
            _ffmpeg_extract_frame_from_end(src, out_path, seconds_from_end=sec, force=True)
            if out_path.exists() and out_path.stat().st_size > 0:
                return out_path
            try:
                out_path.unlink(missing_ok=True)
            except Exception:
                pass
        except Exception as e:
            last_err = e
            try:
                out_path.unlink(missing_ok=True)
            except Exception:
                pass
            continue

    if last_err:
        raise last_err
    raise SystemExit(f"Failed to extract chaining frame from: {src}")


def generate_gemini_image(
    *,
    client: GeminiClient | None,
    model: str,
    prompt: str,
    aspect_ratio: str,
    image_size: str,
    reference_images: list[Path] | None,
    out_path: Path,
    force: bool,
    log_path: Path | None,
    dry_run: bool,
) -> None:
    if out_path.exists() and not force:
        return

    if dry_run:
        print(f"[dry-run] IMAGE {out_path} <- {model} ({aspect_ratio}, {image_size})")
        return

    if client is None:
        raise SystemExit("Gemini client not configured (missing GEMINI_API_KEY).")

    try:
        image_bytes, mime_type, resp = client.generate_image(
            prompt=prompt,
            aspect_ratio=aspect_ratio,
            image_size=image_size,
            reference_images=reference_images,
            model=model,
        )
    except (HttpError, ValueError) as e:
        raise SystemExit(str(e)) from e

    if log_path:
        log_path.parent.mkdir(parents=True, exist_ok=True)
        redacted = json.loads(json.dumps(resp))
        # redact base64 payloads
        for cand in redacted.get("candidates", []) or []:
            for part in (cand.get("content", {}) or {}).get("parts", []) or []:
                inline = part.get("inlineData") or part.get("inline_data")
                if inline and "data" in inline:
                    inline["data"] = f"<redacted {len(inline['data'])} chars>"
        log_path.write_text(json.dumps(redacted, ensure_ascii=False, indent=2), encoding="utf-8")

    out_path.parent.mkdir(parents=True, exist_ok=True)

    suffix = _guess_image_suffix(mime_type)
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
        tmp_path = Path(tmp.name)
        tmp.write(image_bytes)

    try:
        try:
            _run(
                [
                    "ffmpeg",
                    "-hide_banner",
                    "-y",
                    "-i",
                    str(tmp_path),
                    "-frames:v",
                    "1",
                    "-update",
                    "1",
                    str(out_path),
                ]
            )
        except FileNotFoundError:
            out_path.write_bytes(image_bytes)
    finally:
        try:
            tmp_path.unlink(missing_ok=True)
        except Exception:
            pass


def generate_seadream_image(
    *,
    client: SeaDreamClient | None,
    model: str,
    prompt: str,
    size: str,
    out_path: Path,
    force: bool,
    log_path: Path | None,
    dry_run: bool,
) -> None:
    if out_path.exists() and not force:
        return

    if dry_run:
        print(f"[dry-run] IMAGE {out_path} <- {model} (size={size})")
        return

    if client is None:
        raise SystemExit("SeaDream client not configured (missing SEADREAM_API_KEY).")

    try:
        image_bytes, mime_type, resp = client.generate_image(prompt=prompt, size=size, model=model)
    except (HttpError, ValueError) as e:
        raise SystemExit(str(e)) from e

    if log_path:
        log_path.parent.mkdir(parents=True, exist_ok=True)
        redacted = json.loads(json.dumps(resp))
        for item in redacted.get("data", []) or []:
            if isinstance(item, dict) and "b64_json" in item:
                item["b64_json"] = "<redacted>"
        log_path.write_text(json.dumps(redacted, ensure_ascii=False, indent=2), encoding="utf-8")

    out_path.parent.mkdir(parents=True, exist_ok=True)

    mime_type = mime_type or "image/png"
    suffix = _guess_image_suffix(mime_type)
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
        tmp_path = Path(tmp.name)
        tmp.write(image_bytes)

    try:
        try:
            _run(
                [
                    "ffmpeg",
                    "-hide_banner",
                    "-y",
                    "-i",
                    str(tmp_path),
                    "-frames:v",
                    "1",
                    "-update",
                    "1",
                    str(out_path),
                ]
            )
        except FileNotFoundError:
            out_path.write_bytes(image_bytes)
    finally:
        try:
            tmp_path.unlink(missing_ok=True)
        except Exception:
            pass


def generate_veo_video(
    *,
    client: GeminiClient | None,
    model: str,
    prompt: str,
    negative_prompt: str,
    duration_seconds: int,
    aspect_ratio: str,
    resolution: str,
    input_image: Path | None,
    last_frame_image: Path | None,
    reference_images: list[Path] | None,
    out_path: Path,
    poll_every: float,
    timeout_seconds: float,
    force: bool,
    log_path: Path | None,
    dry_run: bool,
) -> None:
    if out_path.exists() and not force:
        return

    if dry_run:
        kind = "F2F" if (input_image and last_frame_image) else ("I2V" if input_image else "T2V")
        print(f"[dry-run] VIDEO({kind}) {out_path} <- {model} ({duration_seconds}s, {aspect_ratio}, {resolution})")
        return

    # Use the official Google GenAI Python SDK for Veo.
    # This supports first-frame image + lastFrame constraint via config.
    try:
        from google import genai  # type: ignore
        from google.genai import types  # type: ignore
    except Exception as e:
        raise SystemExit(
            "google-genai is required for Veo video generation.\n"
            "Install: pip install google-genai"
        ) from e

    api_key = _env("GEMINI_API_KEY")
    if not api_key:
        raise SystemExit("Missing GEMINI_API_KEY (required for Veo video generation).")

    if input_image is None:
        raise SystemExit("Veo image-to-video requires a first-frame image (input_image).")

    client_sdk = genai.Client(api_key=api_key)

    first = types.Image.from_file(location=str(input_image))
    last = types.Image.from_file(location=str(last_frame_image)) if last_frame_image is not None else None

    refs_cfg: list[types.VideoGenerationReferenceImage] | None = None
    # Some fast/cheap variants may not support reference images; keep this best-effort.
    allow_reference_images = "fast" not in (model or "").lower()
    if reference_images and allow_reference_images:
        refs_cfg = []
        for p in reference_images:
            refs_cfg.append(
                types.VideoGenerationReferenceImage(
                    image=types.Image.from_file(location=str(p)),
                    reference_type=types.VideoGenerationReferenceType.ASSET,
                )
            )

    cfg = types.GenerateVideosConfig(
        number_of_videos=1,
        duration_seconds=int(duration_seconds),
        aspect_ratio=aspect_ratio,
        resolution=resolution,
        last_frame=last,
        negative_prompt=(negative_prompt.strip() or None),
        reference_images=refs_cfg,
    )

    op = client_sdk.models.generate_videos(model=model, prompt=prompt, image=first, config=cfg)

    deadline = time.time() + float(timeout_seconds)
    while not getattr(op, "done", False):
        if time.time() > deadline:
            raise SystemExit(f"Timed out waiting for Veo operation: {getattr(op, 'name', '<unknown>')}")
        time.sleep(float(poll_every))
        op = client_sdk.operations.get(op)

    if getattr(op, "error", None):
        raise SystemExit(f"Veo operation failed: {op.error}")

    if log_path:
        log_path.parent.mkdir(parents=True, exist_ok=True)
        try:
            log_path.write_text(json.dumps(op.model_dump(), ensure_ascii=False, indent=2), encoding="utf-8")  # type: ignore[attr-defined]
        except Exception:
            log_path.write_text(str(op), encoding="utf-8")

    resp = getattr(op, "response", None) or getattr(op, "result", None)
    if resp is None:
        raise SystemExit("Veo operation done but response/result is None.")

    videos = getattr(resp, "generated_videos", None) or getattr(resp, "generatedVideos", None) or []
    if not videos:
        filtered_count = getattr(resp, "rai_media_filtered_count", None) or getattr(resp, "raiMediaFilteredCount", None)
        filtered_reasons = getattr(resp, "rai_media_filtered_reasons", None) or getattr(resp, "raiMediaFilteredReasons", None)
        raise SystemExit(
            "Veo operation completed but generated_videos is empty.\n"
            f"rai_media_filtered_count={filtered_count}\n"
            f"rai_media_filtered_reasons={filtered_reasons}"
        )

    video_obj = getattr(videos[0], "video", None)
    if video_obj is None:
        raise SystemExit("Veo response missing generated_videos[0].video.")

    raw = getattr(video_obj, "video_bytes", None)
    if isinstance(raw, str) and raw:
        import base64

        data = base64.b64decode(raw)
    elif isinstance(raw, (bytes, bytearray)) and raw:
        data = bytes(raw)
    else:
        uri = getattr(video_obj, "uri", None)
        if uri:
            data = request_bytes(url=str(uri), method="GET", headers={"x-goog-api-key": api_key}, timeout_seconds=600.0)
        else:
            raise SystemExit("Veo response video has no video_bytes and no uri.")

    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_bytes(data)


def generate_kling_video(
    *,
    client: KlingClient | None,
    model: str,
    prompt: str,
    negative_prompt: str,
    duration_seconds: int,
    aspect_ratio: str,
    resolution: str,
    input_image: Path | None,
    last_frame_image: Path | None,
    out_path: Path,
    poll_every: float,
    timeout_seconds: float,
    force: bool,
    log_path: Path | None,
    dry_run: bool,
) -> None:
    if out_path.exists() and not force:
        return

    if dry_run:
        kind = "F2F" if (input_image and last_frame_image) else ("I2V" if input_image else "T2V")
        print(f"[dry-run] VIDEO({kind}) {out_path} <- {model} ({duration_seconds}s, {aspect_ratio}, {resolution})")
        return

    if client is None:
        raise SystemExit("Kling client not configured (missing KLING_API_KEY).")

    try:
        submit = client.start_video_generation(
            prompt=prompt,
            duration_seconds=int(duration_seconds),
            aspect_ratio=aspect_ratio,
            resolution=resolution,
            input_image=input_image,
            last_frame_image=last_frame_image,
            negative_prompt=(negative_prompt.strip() or None),
            model=model,
            timeout_seconds=180.0,
        )
        operation_id = client.extract_operation_id(submit)
        op = client.poll_operation(
            operation_id_or_url=operation_id,
            poll_every_seconds=float(poll_every),
            timeout_seconds=float(timeout_seconds),
        )
    except (HttpError, TimeoutError, ValueError) as e:
        raise SystemExit(str(e)) from e

    if log_path:
        log_path.parent.mkdir(parents=True, exist_ok=True)
        log_path.write_text(json.dumps({"submit": submit, "operation": op}, ensure_ascii=False, indent=2), encoding="utf-8")

    if client.is_failed_operation(op):
        raise SystemExit(f"Kling operation failed: {json.dumps(op, ensure_ascii=False)}")

    try:
        video_uri = client.extract_video_uri(op)
        client.download_to_file(uri=video_uri, out_path=out_path)
    except (HttpError, ValueError) as e:
        raise SystemExit(str(e)) from e


def normalize_tool_name(tool: str | None) -> str:
    if not tool:
        return ""
    return tool.strip().lower().replace(" ", "_")


def resolve_path(base_dir: Path, maybe_path: str | None) -> Path | None:
    if not maybe_path:
        return None
    p = Path(maybe_path)
    return p if p.is_absolute() else (base_dir / p)


def main() -> None:
    load_env_files(repo_root=REPO_ROOT)

    parser = argparse.ArgumentParser(description="Generate assets from a video manifest.")
    parser.add_argument("--manifest", required=True, help="Path to video_manifest.md")
    parser.add_argument("--base-dir", default=None, help="Resolve relative paths from this dir (default: manifest dir).")
    parser.add_argument("--force", action="store_true", help="Overwrite existing outputs.")
    parser.add_argument("--dry-run", action="store_true", help="Plan only (no API calls).")

    parser.add_argument("--skip-images", action="store_true")
    parser.add_argument("--skip-videos", action="store_true")
    parser.add_argument("--skip-audio", action="store_true")

    parser.add_argument("--scene-ids", default=None, help='Comma-separated list like "1,3,5" (default: all).')

    # Gemini Image
    parser.add_argument("--gemini-api-base", default=_env("GEMINI_API_BASE", "https://generativelanguage.googleapis.com/v1beta"))
    parser.add_argument("--gemini-api-key", default=_env("GEMINI_API_KEY"))
    parser.add_argument("--gemini-image-model", default=_env("GEMINI_IMAGE_MODEL", "gemini-3-pro-image-preview"))
    parser.add_argument("--image-size", default="2K")
    parser.add_argument("--image-aspect-ratio", default=None)
    parser.add_argument("--image-prompt-prefix", default="", help="Optional text prepended to every image prompt.")
    parser.add_argument("--image-prompt-suffix", default="", help="Optional text appended to every image prompt.")
    parser.add_argument(
        "--apply-asset-guides",
        action="store_true",
        help="Merge manifest assets.character_bible/style_guide into per-scene prompts/references (best-effort).",
    )
    parser.add_argument(
        "--asset-guides-character-refs",
        choices=["scene", "auto", "all", "none"],
        default="auto",
        help='When applying asset guides, how to add character_bible.reference_images to each scene ("auto"=only if exactly 1 character).',
    )
    parser.add_argument(
        "--log-prompts",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="Write the final prompts to the provider log dir for reproducibility.",
    )
    parser.add_argument(
        "--require-character-ids",
        action="store_true",
        help="When using --apply-asset-guides with --asset-guides-character-refs scene, require explicit character_ids per scene (use [] for B-roll).",
    )
    parser.add_argument(
        "--require-object-ids",
        action="store_true",
        help="When using --apply-asset-guides with assets.object_bible, require explicit object_ids per scene (use [] when none).",
    )
    parser.add_argument(
        "--require-object-reference-scenes",
        action="store_true",
        help="When assets.object_bible is present, require that each reference_images path is generated by some scene output.",
    )
    parser.add_argument(
        "--character-reference-views",
        default="",
        help='For character reference scenes (assets/characters/*.png), also generate additional view images. Comma-separated: "front,side,back". Default: disabled.',
    )
    parser.add_argument(
        "--character-reference-strip",
        action=argparse.BooleanOptionalAction,
        default=False,
        help="When generating character reference views, also create a single horizontal strip image (front|side|back) for video references.",
    )
    parser.add_argument(
        "--character-reference-strip-suffix",
        default="_refstrip",
        help='Suffix for the strip image filename (default: "_refstrip").',
    )
    parser.add_argument(
        "--video-reference-prefer-character-refstrips",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="When generating videos, prefer the combined character ref strip images (if present) over individual character view refs.",
    )
    parser.add_argument(
        "--image-batch-size",
        type=int,
        default=0,
        help="Generate image scenes in batches of N (story scenes only; character ref scenes may be included automatically).",
    )
    parser.add_argument(
        "--image-batch-index",
        type=int,
        default=1,
        help="1-based batch index for --image-batch-size (e.g., size=10 index=1 generates the first 10 story scenes).",
    )
    parser.add_argument(
        "--image-batch-include-character-refs",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="When using --image-batch-size, also generate missing character reference images (assets/characters/*) in the same run.",
    )

    # SeaDream (Seedream 4.5, OpenAI Images compatible)
    parser.add_argument("--seadream-api-base", default=_env("SEADREAM_API_BASE", "https://ark.ap-southeast.bytepluses.com/api/v3"))
    parser.add_argument("--seadream-api-key", default=_env("SEADREAM_API_KEY"))
    parser.add_argument("--seadream-model", default=_env("SEADREAM_MODEL", "seedream-4-5-251128"))
    parser.add_argument("--seadream-size", default=_env("SEADREAM_SIZE", "1024x1536"))

    # Veo
    parser.add_argument("--gemini-video-model", default=_env("GEMINI_VIDEO_MODEL", "veo-3.1-fast-generate-preview"))
    parser.add_argument("--video-resolution", default="720p")
    parser.add_argument("--video-aspect-ratio", default=None)
    parser.add_argument("--default-scene-seconds", type=int, default=6)
    parser.add_argument("--video-prompt-prefix", default="", help="Optional text prepended to every video prompt.")
    parser.add_argument("--video-prompt-suffix", default="", help="Optional text appended to every video prompt.")
    parser.add_argument(
        "--video-negative-prompt",
        default="",
        help="Negative prompt for video generation (provider-dependent).",
    )
    parser.add_argument("--poll-every", type=float, default=5.0)
    parser.add_argument("--timeout-seconds", type=float, default=900.0)
    parser.add_argument(
        "--enable-last-frame",
        action="store_true",
        help="Try to pass last-frame conditioning using manifest last_frame (best-effort; provider-dependent).",
    )
    parser.add_argument(
        "--chain-first-frame-from-prev-video",
        action="store_true",
        help="Use a frame extracted from the previous scene's video as the next video's first frame (improves seamless joins).",
    )
    parser.add_argument(
        "--chain-first-frame-seconds-from-end",
        type=float,
        default=1.0,
        help="When chaining, extract the first frame from this many seconds before the end of the previous video.",
    )

    # Kling
    parser.add_argument("--kling-api-base", default=_env("KLING_API_BASE", "https://api.klingai.com"))
    parser.add_argument("--kling-api-key", default=_env("KLING_API_KEY"))
    parser.add_argument("--kling-video-model", default=_env("KLING_VIDEO_MODEL", "kling-3.0"))

    # logging
    parser.add_argument("--log-dir", default=None, help="Directory to write provider logs (default: <base>/logs/providers).")

    # ElevenLabs
    parser.add_argument("--elevenlabs-api-key", default=_env("ELEVENLABS_API_KEY"))
    parser.add_argument("--elevenlabs-api-base", default=_env("ELEVENLABS_API_BASE", "https://api.elevenlabs.io/v1"))
    parser.add_argument("--elevenlabs-voice-id", default=_env("ELEVENLABS_VOICE_ID", DEFAULT_ELEVENLABS_VOICE_ID))
    parser.add_argument("--elevenlabs-model-id", default=_env("ELEVENLABS_MODEL_ID", "eleven_multilingual_v2"))
    parser.add_argument("--elevenlabs-output-format", default=_env("ELEVENLABS_OUTPUT_FORMAT", "mp3_44100_128"))
    parser.add_argument("--tts-prompt-prefix", default="", help="Optional text prepended to every TTS input.")
    parser.add_argument("--tts-prompt-suffix", default="", help="Optional text appended to every TTS input.")

    args = parser.parse_args()

    manifest_path = Path(args.manifest)
    if not manifest_path.exists():
        raise SystemExit(f"Manifest not found: {manifest_path}")

    base_dir = Path(args.base_dir) if args.base_dir else manifest_path.parent
    md = manifest_path.read_text(encoding="utf-8")
    yaml_text = extract_yaml_block(md)
    metadata, guides, scenes = parse_manifest_yaml_full(yaml_text)

    char_views = sorted(_parse_csv_set(args.character_reference_views))
    allowed_views = {"front", "side", "back"}
    unknown = [v for v in char_views if v not in allowed_views]
    if unknown:
        raise SystemExit(f"Unknown --character-reference-views values: {unknown}. Allowed: front,side,back")
    # If the user asks for a ref strip, we must have all three views.
    if args.character_reference_strip:
        char_views = sorted(set(char_views) | {"front", "side", "back"})

    # Expand character_bible reference_images to include derived view/strip filenames (opt-in).
    # This keeps existing manifests compatible while letting story scenes automatically reference
    # the additional turnaround views when using --apply-asset-guides.
    if args.apply_asset_guides and (char_views or args.character_reference_strip):
        expanded_cb: list[CharacterBibleEntry] = []
        for entry in guides.character_bible or []:
            refs = _dedupe_keep_order(list(entry.reference_images or []))
            extra: list[str] = []
            for ref in refs:
                ref_p = Path(ref)
                # only expand for assets/characters/* references
                if "assets" not in ref_p.parts or "characters" not in ref_p.parts:
                    continue
                # derive views
                for v in char_views:
                    if v == "front":
                        continue
                    extra.append(str(_derive_character_view_path(ref_p, v)))
                if args.character_reference_strip:
                    extra.append(str(_derive_character_refstrip_path(ref_p, args.character_reference_strip_suffix)))
            expanded = _dedupe_keep_order(refs + extra)
            expanded_cb.append(
                CharacterBibleEntry(
                    character_id=entry.character_id,
                    reference_images=expanded,
                    fixed_prompts=list(entry.fixed_prompts or []),
                    notes=entry.notes,
                )
            )
        guides = AssetGuides(character_bible=expanded_cb, style_guide=guides.style_guide, object_bible=guides.object_bible)

    if args.apply_asset_guides:
        if yaml is None:
            raise SystemExit("PyYAML is required for --apply-asset-guides (dependency: pyyaml).")
        if not guides.character_bible and guides.style_guide is None:
            print("[warn] --apply-asset-guides: no assets.character_bible/style_guide found in manifest.")
        if len(guides.character_bible) > 1 and str(args.asset_guides_character_refs).strip().lower() == "auto":
            print(
                "[warn] --apply-asset-guides: assets.character_bible has multiple entries; "
                "character refs will not be auto-added in 'auto' mode. "
                "Use --asset-guides-character-refs all to force."
            )
        for scene in scenes:
            apply_asset_guides_to_scene(scene=scene, guides=guides, character_refs_mode=args.asset_guides_character_refs)

    if not scenes:
        raise SystemExit("No scenes found in manifest YAML.")

    scene_filter: set[int] | None = None
    if args.scene_ids:
        scene_filter = {int(x.strip()) for x in args.scene_ids.split(",") if x.strip()}

    validate_scene_character_ids(
        scenes=scenes,
        require=bool(args.require_character_ids),
        mode=args.asset_guides_character_refs,
        scene_filter=scene_filter,
    )
    validate_scene_object_ids(
        scenes=scenes,
        guides=guides,
        require=bool(args.require_object_ids),
        scene_filter=scene_filter,
    )
    validate_object_reference_scenes(
        scenes=scenes,
        guides=guides,
        require=bool(args.require_object_reference_scenes),
    )

    aspect_ratio = (
        args.image_aspect_ratio
        or args.video_aspect_ratio
        or (metadata.get("aspect_ratio") if isinstance(metadata.get("aspect_ratio"), str) else None)
        or "9:16"
    )

    log_dir = Path(args.log_dir) if args.log_dir else (base_dir / "logs/providers")

    def _scene_uses_tool(scene: Scene, tools: set[str]) -> bool:
        return normalize_tool_name(scene.image_tool) in tools

    needs_gemini_image = (
        not args.skip_images
        and any(
            _scene_uses_tool(scene, {"google_nanobanana_pro", "nanobanana_pro"})
            and scene.image_output
            and scene.image_prompt
            and (scene_filter is None or scene.scene_id in scene_filter)
            for scene in scenes
        )
    )
    needs_seadream_image = (
        not args.skip_images
        and any(
            _scene_uses_tool(scene, {"seadream", "seedream", "seedream_4_5", "byteplus_seedream_4_5"})
            and scene.image_output
            and scene.image_prompt
            and (scene_filter is None or scene.scene_id in scene_filter)
            for scene in scenes
        )
    )
    needs_gemini_video = (
        not args.skip_videos
        and any(
            normalize_tool_name(scene.video_tool) == "google_veo_3_1"
            and scene.video_output
            and (scene_filter is None or scene.scene_id in scene_filter)
            for scene in scenes
        )
    )
    needs_kling_video = (
        not args.skip_videos
        and any(
            normalize_tool_name(scene.video_tool) in {"kling_3_0", "kling"}
            and scene.video_output
            and (scene_filter is None or scene.scene_id in scene_filter)
            for scene in scenes
        )
    )

    gemini_client: GeminiClient | None = None
    if not args.dry_run and (needs_gemini_image or needs_gemini_video):
        if not args.gemini_api_key:
            raise SystemExit("Missing GEMINI_API_KEY (required for Gemini image/video).")
        gemini_client = GeminiClient(
            GeminiConfig(
                api_key=args.gemini_api_key,
                api_base=args.gemini_api_base,
                image_model=args.gemini_image_model,
                video_model=args.gemini_video_model,
            )
        )

    kling_client: KlingClient | None = None
    if not args.dry_run and needs_kling_video:
        if not args.kling_api_key:
            raise SystemExit("Missing KLING_API_KEY (required for Kling video generation).")
        kling_client = KlingClient(
            KlingConfig(
                api_key=args.kling_api_key,
                api_base=args.kling_api_base,
                video_model=args.kling_video_model,
            )
        )

    seadream_client: SeaDreamClient | None = None
    if not args.dry_run and needs_seadream_image:
        if not args.seadream_api_key:
            raise SystemExit("Missing SEADREAM_API_KEY (required for SeaDream image generation).")
        seadream_client = SeaDreamClient(
            SeaDreamConfig(
                api_key=args.seadream_api_key,
                api_base=args.seadream_api_base,
                image_model=args.seadream_model,
            )
        )

    elevenlabs_client: ElevenLabsClient | None = None
    if not args.dry_run and not args.skip_audio:
        needs_elevenlabs = any(
            normalize_tool_name(scene.narration_tool) == "elevenlabs"
            and scene.narration_output
            and (scene_filter is None or scene.scene_id in scene_filter)
            for scene in scenes
        )
        if needs_elevenlabs:
            if not args.elevenlabs_api_key:
                raise SystemExit("Missing ELEVENLABS_API_KEY (required for ElevenLabs TTS).")
            voice_id = str(args.elevenlabs_voice_id or "").strip()
            if not voice_id:
                voice_id = DEFAULT_ELEVENLABS_VOICE_ID
            if voice_id.lower() in {"your_voice_id", "voice_id_tbd", "tbd"}:
                print(
                    "[warn] ELEVENLABS_VOICE_ID looks like a placeholder; falling back to default voice_id "
                    f"({DEFAULT_ELEVENLABS_VOICE_ID})."
                )
                voice_id = DEFAULT_ELEVENLABS_VOICE_ID
            args.elevenlabs_voice_id = voice_id
            elevenlabs_client = ElevenLabsClient(
                ElevenLabsConfig(
                    api_key=args.elevenlabs_api_key,
                    api_base=args.elevenlabs_api_base,
                    voice_id=voice_id,
                    model_id=args.elevenlabs_model_id,
                    output_format=args.elevenlabs_output_format,
                )
            )

    # Pass 1: images (allows later videos to reference other scene images, e.g. first/last frame conditioning).
    image_scenes: list[SceneSpec] = []
    for scene in scenes:
        if scene_filter is not None and scene.scene_id not in scene_filter:
            continue
        if args.skip_images or not scene.image_output or not scene.image_prompt:
            continue
        image_scenes.append(scene)

    # Ensure reference images are generated first so later scenes can safely reference them.
    def _image_scene_sort_key(s: SceneSpec) -> int:
        outp = resolve_path(base_dir, s.image_output) if s.image_output else None
        if outp and _is_character_ref_path(outp):
            return 0
        if outp and _is_object_ref_path(outp):
            return 1
        return 2

    image_scenes.sort(key=_image_scene_sort_key)

    if args.image_batch_size:
        if int(args.image_batch_size) <= 0:
            raise SystemExit("--image-batch-size must be a positive integer.")
        if int(args.image_batch_index) <= 0:
            raise SystemExit("--image-batch-index must be >= 1.")

        char_ref_scenes: list[SceneSpec] = []
        obj_ref_scenes: list[SceneSpec] = []
        story_scenes: list[SceneSpec] = []
        for s in image_scenes:
            outp = resolve_path(base_dir, s.image_output) if s.image_output else None
            if outp and _is_character_ref_path(outp):
                char_ref_scenes.append(s)
            elif outp and _is_object_ref_path(outp):
                obj_ref_scenes.append(s)
            else:
                story_scenes.append(s)

        start = (int(args.image_batch_index) - 1) * int(args.image_batch_size)
        end = start + int(args.image_batch_size)
        selected_story = story_scenes[start:end]

        selected: list[SceneSpec] = []
        if args.image_batch_include_character_refs:
            for s in char_ref_scenes:
                outp = resolve_path(base_dir, s.image_output) if s.image_output else None
                if not outp:
                    continue
                # Avoid re-calling paid APIs unnecessarily; include only missing refs unless --force.
                if args.force or args.dry_run or (not outp.exists()):
                    selected.append(s)
            for s in obj_ref_scenes:
                outp = resolve_path(base_dir, s.image_output) if s.image_output else None
                if not outp:
                    continue
                if args.force or args.dry_run or (not outp.exists()):
                    selected.append(s)
        selected.extend(selected_story)
        image_scenes = selected

    for scene in image_scenes:
        if scene_filter is not None and scene.scene_id not in scene_filter:
            continue

        if args.skip_images or not scene.image_output or not scene.image_prompt:
            continue

        tool = normalize_tool_name(scene.image_tool)
        out_path = resolve_path(base_dir, scene.image_output)
        if not out_path:
            raise SystemExit(f"scene{scene.scene_id}: missing image output path")

        scene_aspect_ratio = scene.image_aspect_ratio or aspect_ratio
        scene_image_size = scene.image_size or args.image_size

        refs: list[Path] = []
        for ref_str in scene.image_references or []:
            ref_path = resolve_path(base_dir, ref_str)
            if not ref_path:
                continue
            if not args.dry_run and not ref_path.exists():
                raise SystemExit(f"scene{scene.scene_id}: reference image not found: {ref_path}")
            refs.append(ref_path)

        is_char_ref = bool(out_path and _is_character_ref_path(out_path))

        if tool in {"google_nanobanana_pro", "nanobanana_pro"}:
            prefix = (args.image_prompt_prefix or "").strip()
            suffix = (args.image_prompt_suffix or "").strip()
            prompt = scene.image_prompt.strip()
            if prefix:
                prompt = prefix + "\n\n" + prompt
            if suffix:
                prompt = prompt + "\n\n" + suffix

            if is_char_ref and (char_views or args.character_reference_strip):
                # Turnaround: generate front/side/back images + optional ref strip.
                views_to_generate = [v for v in ("front", "side", "back") if (v == "front" or v in char_views)]
                if "front" not in views_to_generate:
                    views_to_generate.insert(0, "front")

                view_paths: dict[str, Path] = {"front": out_path}
                for v in ("side", "back"):
                    if v in views_to_generate:
                        view_paths[v] = _derive_character_view_path(out_path, v)

                # front first
                front_prompt = _character_view_prompt(prompt, "front")
                if args.log_prompts:
                    log_dir.mkdir(parents=True, exist_ok=True)
                    (log_dir / f"scene{scene.scene_id}_image_prompt.txt").write_text(front_prompt + "\n", encoding="utf-8")
                generate_gemini_image(
                    client=gemini_client,
                    model=args.gemini_image_model,
                    prompt=front_prompt,
                    aspect_ratio=scene_aspect_ratio,
                    image_size=scene_image_size,
                    reference_images=refs,
                    out_path=view_paths["front"],
                    force=args.force,
                    log_path=log_dir / f"scene{scene.scene_id}_image.json",
                    dry_run=args.dry_run,
                )

                # side/back conditioned by the front reference when available
                conditioned_refs = list(refs)
                if view_paths["front"] not in conditioned_refs:
                    conditioned_refs.append(view_paths["front"])

                for v in ("side", "back"):
                    if v not in view_paths:
                        continue
                    vprompt = _character_view_prompt(prompt, v)
                    if args.log_prompts:
                        log_dir.mkdir(parents=True, exist_ok=True)
                        (log_dir / f"scene{scene.scene_id}_image_prompt_{v}.txt").write_text(vprompt + "\n", encoding="utf-8")
                    generate_gemini_image(
                        client=gemini_client,
                        model=args.gemini_image_model,
                        prompt=vprompt,
                        aspect_ratio=scene_aspect_ratio,
                        image_size=scene_image_size,
                        reference_images=conditioned_refs,
                        out_path=view_paths[v],
                        force=args.force,
                        log_path=log_dir / f"scene{scene.scene_id}_image_{v}.json",
                        dry_run=args.dry_run,
                    )

                if args.character_reference_strip and all(k in view_paths for k in ("front", "side", "back")):
                    strip_path = _derive_character_refstrip_path(out_path, args.character_reference_strip_suffix)
                    if not args.dry_run:
                        _ffmpeg_hstack_images(
                            [view_paths["front"], view_paths["side"], view_paths["back"]],
                            strip_path,
                            force=args.force,
                        )
                    else:
                        print(f"[dry-run] IMAGE {strip_path} <- hstack(front,side,back)")
                continue

            if args.log_prompts:
                log_dir.mkdir(parents=True, exist_ok=True)
                (log_dir / f"scene{scene.scene_id}_image_prompt.txt").write_text(prompt + "\n", encoding="utf-8")
            generate_gemini_image(
                client=gemini_client,
                model=args.gemini_image_model,
                prompt=prompt,
                aspect_ratio=scene_aspect_ratio,
                image_size=scene_image_size,
                reference_images=refs,
                out_path=out_path,
                force=args.force,
                log_path=log_dir / f"scene{scene.scene_id}_image.json",
                dry_run=args.dry_run,
            )
        elif tool in {"seadream", "seedream", "seedream_4_5", "byteplus_seedream_4_5"}:
            base_prompt = scene.image_prompt.strip()
            if is_char_ref and (char_views or args.character_reference_strip):
                views_to_generate = [v for v in ("front", "side", "back") if (v == "front" or v in char_views)]
                if "front" not in views_to_generate:
                    views_to_generate.insert(0, "front")
                view_paths: dict[str, Path] = {"front": out_path}
                for v in ("side", "back"):
                    if v in views_to_generate:
                        view_paths[v] = _derive_character_view_path(out_path, v)

                for v in views_to_generate:
                    vprompt = _character_view_prompt(base_prompt, v)
                    if args.log_prompts:
                        log_dir.mkdir(parents=True, exist_ok=True)
                        suffix_name = "" if v == "front" else f"_{v}"
                        (log_dir / f"scene{scene.scene_id}_image_prompt{suffix_name}.txt").write_text(vprompt + "\n", encoding="utf-8")
                    generate_seadream_image(
                        client=seadream_client,
                        model=args.seadream_model,
                        prompt=vprompt,
                        size=args.seadream_size,
                        out_path=view_paths[v],
                        force=args.force,
                        log_path=log_dir / f"scene{scene.scene_id}_image{'' if v == 'front' else '_' + v}.json",
                        dry_run=args.dry_run,
                    )

                if args.character_reference_strip and all(k in view_paths for k in ("front", "side", "back")):
                    strip_path = _derive_character_refstrip_path(out_path, args.character_reference_strip_suffix)
                    if not args.dry_run:
                        _ffmpeg_hstack_images(
                            [view_paths["front"], view_paths["side"], view_paths["back"]],
                            strip_path,
                            force=args.force,
                        )
                    else:
                        print(f"[dry-run] IMAGE {strip_path} <- hstack(front,side,back)")
                continue

            if args.log_prompts:
                log_dir.mkdir(parents=True, exist_ok=True)
                (log_dir / f"scene{scene.scene_id}_image_prompt.txt").write_text(base_prompt + "\n", encoding="utf-8")
            generate_seadream_image(
                client=seadream_client,
                model=args.seadream_model,
                prompt=base_prompt,
                size=args.seadream_size,
                out_path=out_path,
                force=args.force,
                log_path=log_dir / f"scene{scene.scene_id}_image.json",
                dry_run=args.dry_run,
            )
        else:
            raise SystemExit(f"scene{scene.scene_id}: unsupported image tool: {scene.image_tool}")

    # Pass 2: videos
    video_scenes_in_order: list[SceneSpec] = []
    for s in scenes:
        if scene_filter is not None and s.scene_id not in scene_filter:
            continue
        if args.skip_videos or not s.video_output or not (s.video_motion_prompt or s.image_prompt):
            continue
        video_scenes_in_order.append(s)

    video_scene_index_by_id: dict[int, int] = {int(s.scene_id): idx for idx, s in enumerate(video_scenes_in_order)}

    prev_chain_first_frame: Path | None = None
    for scene in scenes:
        if scene_filter is not None and scene.scene_id not in scene_filter:
            continue

        if args.skip_videos or not scene.video_output or not (scene.video_motion_prompt or scene.image_prompt):
            continue

        tool = normalize_tool_name(scene.video_tool)
        out_path = resolve_path(base_dir, scene.video_output)
        if not out_path:
            raise SystemExit(f"scene{scene.scene_id}: missing video output path")

        dur = duration_from_timestamp_range(scene.timestamp, args.default_scene_seconds)

        input_image = resolve_path(base_dir, scene.video_first_frame or scene.video_input_image)
        if input_image is None and scene.image_output:
            input_image = resolve_path(base_dir, scene.image_output)
        if args.chain_first_frame_from_prev_video and prev_chain_first_frame is not None:
            # Best-effort: override the provided first_frame so the new clip starts
            # exactly where the previous clip ended (improves mp4 concat continuity).
            input_image = prev_chain_first_frame
        elif args.chain_first_frame_from_prev_video and prev_chain_first_frame is None:
            # If the user is regenerating only later scenes, we may have skipped the previous video generation.
            # Don't assume contiguous numeric IDs; use manifest order to find the previous video scene.
            idx = video_scene_index_by_id.get(int(scene.scene_id))
            if idx is not None and idx > 0:
                prev_scene = video_scenes_in_order[idx - 1]
                prev_video = resolve_path(base_dir, prev_scene.video_output)
                if prev_video and prev_video.exists() and not args.dry_run:
                    chain_frame = prev_video.with_name(prev_video.stem + "_chain_first_frame.png")
                    try:
                        prev_chain_first_frame = _ffmpeg_extract_frame_from_end_best_effort(
                            prev_video,
                            chain_frame,
                            seconds_from_end=float(args.chain_first_frame_seconds_from_end),
                            force=True,
                        )
                        input_image = prev_chain_first_frame
                    except FileNotFoundError:
                        prev_chain_first_frame = None
        if input_image and not args.dry_run and not input_image.exists():
            raise SystemExit(f"scene{scene.scene_id}: first frame image not found: {input_image}")

        last_image: Path | None = None
        if args.enable_last_frame:
            last_image = resolve_path(base_dir, scene.video_last_frame)
            if last_image and not args.dry_run and not last_image.exists():
                raise SystemExit(f"scene{scene.scene_id}: last frame image not found: {last_image}")

        prompt_parts: list[str] = []
        if scene.video_motion_prompt:
            prompt_parts.append(scene.video_motion_prompt.strip())
        if scene.image_prompt:
            prompt_parts.append("シーン説明:\n" + scene.image_prompt.strip())
        prompt = "\n\n".join(prompt_parts).strip()
        vprefix = (args.video_prompt_prefix or "").strip()
        vsuffix = (args.video_prompt_suffix or "").strip()
        if vprefix:
            prompt = vprefix + "\n\n" + prompt
        if vsuffix:
            prompt = prompt + "\n\n" + vsuffix
        if args.log_prompts:
            log_dir.mkdir(parents=True, exist_ok=True)
            (log_dir / f"scene{scene.scene_id}_video_prompt.txt").write_text(prompt + "\n", encoding="utf-8")

        video_ref_paths: list[Path] = []
        for ref_str in scene.image_references or []:
            ref_path = resolve_path(base_dir, ref_str)
            if not ref_path:
                continue
            if not args.dry_run and not ref_path.exists():
                raise SystemExit(f"scene{scene.scene_id}: reference image not found: {ref_path}")
            video_ref_paths.append(ref_path)

        if args.video_reference_prefer_character_refstrips:
            non_char = [p for p in video_ref_paths if not _is_character_ref_path(p)]
            char = [p for p in video_ref_paths if _is_character_ref_path(p)]
            strips = [p for p in char if _is_character_refstrip_path(p, args.character_reference_strip_suffix)]
            # If any strips are available, keep only strips for character references (reduces token/bandwidth and
            # matches the intended "turnaround strip for video" workflow). Keep other non-character refs intact.
            if strips:
                video_ref_paths = non_char + strips

        if tool == "google_veo_3_1":
            segs, trim_to = _plan_veo_segments(dur)
            if len(segs) == 1:
                generate_veo_video(
                    client=gemini_client,
                    model=args.gemini_video_model,
                    prompt=prompt,
                    negative_prompt=args.video_negative_prompt or "",
                    duration_seconds=segs[0],
                    aspect_ratio=aspect_ratio,
                    resolution=args.video_resolution,
                    input_image=input_image,
                    last_frame_image=last_image,
                    reference_images=video_ref_paths,
                    out_path=out_path,
                    poll_every=args.poll_every,
                    timeout_seconds=args.timeout_seconds,
                    force=args.force,
                    log_path=log_dir / f"scene{scene.scene_id}_video.json",
                    dry_run=args.dry_run,
                )
            else:
                if args.dry_run:
                    print(f"[dry-run] VIDEO scene{scene.scene_id}: segments={segs} then trim_to={trim_to}")
                else:
                    with tempfile.TemporaryDirectory() as tmpdir:
                        tmpdir_path = Path(tmpdir)
                        seg_paths: list[Path] = []
                        for idx, seg_dur in enumerate(segs, start=1):
                            seg_out = tmpdir_path / f"scene{scene.scene_id}_seg{idx}.mp4"
                            generate_veo_video(
                                client=gemini_client,
                                model=args.gemini_video_model,
                                prompt=prompt,
                                negative_prompt=args.video_negative_prompt or "",
                                duration_seconds=seg_dur,
                                aspect_ratio=aspect_ratio,
                                resolution=args.video_resolution,
                                input_image=input_image,
                                last_frame_image=last_image,
                                reference_images=video_ref_paths,
                                out_path=seg_out,
                                poll_every=args.poll_every,
                                timeout_seconds=args.timeout_seconds,
                                force=True,
                                log_path=log_dir / f"scene{scene.scene_id}_video_seg{idx}.json",
                                dry_run=False,
                            )
                            seg_paths.append(seg_out)

                        concat_path = tmpdir_path / f"scene{scene.scene_id}_concat.mp4"
                        _ffmpeg_concat_videos(seg_paths, concat_path)
                        if trim_to:
                            _ffmpeg_trim_video(concat_path, out_path, int(trim_to))
                        else:
                            out_path.parent.mkdir(parents=True, exist_ok=True)
                            out_path.write_bytes(concat_path.read_bytes())
        elif tool in {"kling_3_0", "kling"}:
            generate_kling_video(
                client=kling_client,
                model=args.kling_video_model,
                prompt=prompt,
                negative_prompt=args.video_negative_prompt or "",
                duration_seconds=int(dur),
                aspect_ratio=aspect_ratio,
                resolution=args.video_resolution,
                input_image=input_image,
                last_frame_image=last_image,
                out_path=out_path,
                poll_every=args.poll_every,
                timeout_seconds=args.timeout_seconds,
                force=args.force,
                log_path=log_dir / f"scene{scene.scene_id}_video.json",
                dry_run=args.dry_run,
            )
        else:
            raise SystemExit(f"scene{scene.scene_id}: unsupported video tool: {scene.video_tool}")

        if args.chain_first_frame_from_prev_video:
            if args.dry_run:
                prev_chain_first_frame = out_path.with_name(out_path.stem + "_chain_first_frame.png")
            else:
                chain_frame = out_path.with_name(out_path.stem + "_chain_first_frame.png")
                try:
                    _ffmpeg_extract_frame_from_end_best_effort(
                        out_path,
                        chain_frame,
                        seconds_from_end=float(args.chain_first_frame_seconds_from_end),
                        force=args.force,
                    )
                    prev_chain_first_frame = chain_frame
                except FileNotFoundError:
                    # ffmpeg missing; chaining can't proceed.
                    prev_chain_first_frame = None

    # Pass 3: audio (TTS)
    for scene in scenes:
        if scene_filter is not None and scene.scene_id not in scene_filter:
            continue

        if args.skip_audio or not scene.narration_output:
            continue

        dur = duration_from_timestamp_range(scene.timestamp, args.default_scene_seconds)
        out_path = resolve_path(base_dir, scene.narration_output)
        if not out_path:
            raise SystemExit(f"scene{scene.scene_id}: missing narration output path")

        tool = normalize_tool_name(scene.narration_tool)
        if tool == "elevenlabs":
            if not scene.narration_text:
                raise SystemExit(f"scene{scene.scene_id}: missing narration text for ElevenLabs TTS")
            tts_text = scene.narration_text.strip()
            tprefix = (args.tts_prompt_prefix or "").strip()
            tsuffix = (args.tts_prompt_suffix or "").strip()
            if tprefix:
                tts_text = tprefix + "\n\n" + tts_text
            if tsuffix:
                tts_text = tts_text + "\n\n" + tsuffix
            normalize_dur = dur if scene.narration_normalize_to_scene_duration else None
            generate_elevenlabs_tts(
                client=elevenlabs_client,
                voice_id=str((args.elevenlabs_voice_id or DEFAULT_ELEVENLABS_VOICE_ID)),
                model_id=args.elevenlabs_model_id or "eleven_multilingual_v2",
                output_format=args.elevenlabs_output_format or "mp3_44100_128",
                text=tts_text,
                out_path=out_path,
                duration_seconds=normalize_dur,
                force=args.force,
                request_log_path=log_dir / f"scene{scene.scene_id}_tts_request.json",
                dry_run=args.dry_run,
            )
        elif tool in {"tbd", ""}:
            if args.dry_run:
                print(f"[dry-run] AUDIO {out_path} <- placeholder (tool={scene.narration_tool})")
            else:
                _ffmpeg_write_silence_mp3(out_path, dur, args.force)
        else:
            raise SystemExit(f"scene{scene.scene_id}: unsupported narration tool: {scene.narration_tool}")

    print("Done.")


if __name__ == "__main__":
    main()
